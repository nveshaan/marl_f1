{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03017295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Track generation: 1219..1528 -> 309-tiles track\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nveshaan/Developer/marl_f1/env/lib/python3.10/site-packages/gymnasium/utils/passive_env_checker.py:158: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'my_policy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 15\u001b[0m\n\u001b[1;32m     10\u001b[0m total_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m     13\u001b[0m   \u001b[38;5;66;03m# The actions have to be of the format (num_agents,3)\u001b[39;00m\n\u001b[1;32m     14\u001b[0m   \u001b[38;5;66;03m# The action format for each car is as in the CarRacing-v0 environment.\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m   action \u001b[38;5;241m=\u001b[39m \u001b[43mmy_policy\u001b[49m(obs)\n\u001b[1;32m     17\u001b[0m   \u001b[38;5;66;03m# Similarly, the structure of this is the same as in CarRacing-v0 with an\u001b[39;00m\n\u001b[1;32m     18\u001b[0m   \u001b[38;5;66;03m# additional dimension for the different agents, i.e.\u001b[39;00m\n\u001b[1;32m     19\u001b[0m   \u001b[38;5;66;03m# obs is of shape (num_agents, 96, 96, 3)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m   \u001b[38;5;66;03m# reward is of shape (num_agents,)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m   \u001b[38;5;66;03m# done is a bool and info is not used (an empty dict).\u001b[39;00m\n\u001b[1;32m     22\u001b[0m   obs, reward, done, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'my_policy' is not defined"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import gym_multi_car_racing\n",
    "\n",
    "env = gym.make(\"MultiCarRacing-v0\", num_agents=2, direction='CCW',\n",
    "        use_random_direction=True, backwards_flag=True, h_ratio=0.25,\n",
    "        use_ego_color=False)\n",
    "\n",
    "obs = env.reset()\n",
    "done = False\n",
    "total_reward = 0\n",
    "\n",
    "while not done:\n",
    "  # The actions have to be of the format (num_agents,3)\n",
    "  # The action format for each car is as in the CarRacing-v0 environment.\n",
    "  action = my_policy(obs)\n",
    "\n",
    "  # Similarly, the structure of this is the same as in CarRacing-v0 with an\n",
    "  # additional dimension for the different agents, i.e.\n",
    "  # obs is of shape (num_agents, 96, 96, 3)\n",
    "  # reward is of shape (num_agents,)\n",
    "  # done is a bool and info is not used (an empty dict).\n",
    "  obs, reward, done, info = env.step(action)\n",
    "  total_reward += reward\n",
    "  env.render()\n",
    "\n",
    "print(\"individual scores:\", total_reward)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
